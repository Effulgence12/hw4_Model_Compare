# 大语言模型(LLM)本地部署实验报告

项目公开可访问链接：
github: https://github.com/Effulgence12/hw4_Model_Compare.git
魔搭平台：https://modelscope.cn/models/Kerlap/Models_Compare_chatglm_qwen/summary

## 1. 项目概述

本项目旨在探索大语言模型(LLM)的本地部署方法及其性能表现。通过在云服务器环境中部署开源大模型，我们可以深入了解 LLM 的运行机制、资源需求和实际应用效果。本次实验选取了两个主流的中文大语言模型：ChatGLM3-6B 和 Qwen-7B-Chat 进行对比测试，通过一系列多领域的问题来评估它们在理解能力、创造力、逻辑推理和专业知识等方面的表现差异。

### 1.1 实验目标

- 掌握大语言模型在 CPU 环境下的部署方法
- 实现两个主流开源大模型的本地化部署
- 通过对比测试分析不同模型的性能特点和适用场景
- 深入理解大语言模型的推理过程和资源消耗情况

### 1.2 技术栈

- Python 3.10
- PyTorch (CPU 版本)
- Transformers 库
- ModelScope 平台
- Conda 环境管理

## 2. 平台搭建

本项目使用 ModelScope 平台提供的云服务器环境进行大模型部署。以下是平台搭建的详细步骤：

1. 访问 ModelScope 官网（https://www.modelscope.cn/home），
   点击右上角完成新用户注册。
2. 登录后进入首页，确认已绑定阿里云账号并获得免费云计算资源。
3. 启动 CPU 服务器实例，用于后续的模型部署。

![](assets/1.png)

4. 进入个人主页，查看免费实例（CPU 环境）。
5. 点击 Terminal 图标打开终端命令行环境，准备进行环境配置。

## 3. 环境搭建

### 3.1 conda 环境配置

由于大语言模型对环境依赖较为复杂，我们使用 Conda 创建独立的 Python 环境，以避免依赖冲突问题。

1. **手动安装 conda**（若 CPU 版本未预装）：

   ```bash
   cd /opt/conda/envs
   wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
   bash Miniconda3-latest-Linux-x86_64.sh -b -p /opt/conda
   echo 'export PATH="/opt/conda/bin:$PATH"' >> ~/.bashrc
   source ~/.bashrc
   conda --version
   ```

   ![](assets/2.png)

2. **创建并激活环境**：
   ```bash
   conda create -n qwen_env python=3.10 -y
   source /opt/conda/etc/profile.d/conda.sh
   conda activate qwen_env
   ```
   ![](assets/3.png)![](assets/4.png)

### 3.2 基础依赖安装

为了支持大模型的加载和推理，需要安装一系列依赖包，包括 PyTorch、Transformers 等核心库。

1. **检查 pip 联网状态并更新基础工具**：

   ```bash
   pip install -U pip setuptools wheel
   ```

2. **安装 PyTorch CPU 版本**：

   由于我们在 CPU 环境下运行，选择 PyTorch 的 CPU 版本可以减少不必要的 CUDA 依赖，提高兼容性。

   ```bash
   pip install \
   torch==2.3.0+cpu \
   torchvision==0.18.0+cpu \
   --index-url https://download.pytorch.org/whl/cpu
   ```

   ![](assets/5.png)

3. **安装兼容依赖包**：

   以下依赖包是运行大语言模型所必需的，包括模型加载、量化优化和推理加速等功能。

   ```bash
   pip install \
   "intel-extension-for-transformers==1.4.2" \
   "neural-compressor==2.5" \
   "transformers==4.33.3" \
   "modelscope==1.9.5" \
   "pydantic==1.10.13" \
   "sentencepiece" \
   "tiktoken" \
   "einops" \
   "transformers_stream_generator" \
   "uvicorn" \
   "fastapi" \
   "yacs" \
   "setuptools_scm"
   ```

   安装 fschat（需启用 PEP517 构建）：

   ```bash
   pip install fschat --use-pep517
   ```

   可选增强工具：

   ```bash
   pip install tqdm huggingface-hub
   ```

## 4. 大模型实践操作

### 4.1 下载大模型到本地

本实验选择了两个主流的开源中文大语言模型：ChatGLM3-6B 和 Qwen-7B-Chat。这两个模型都具有较好的中文理解能力，且在 CPU 环境下可以勉强运行。

1. **切换至数据目录**：

   ```bash
   cd /mnt/data
   ```

2. **下载模型**（分别下载两个模型）：
   ```bash
   # Qwen-7B-Chat
   git clone https://www.modelscope.cn/qwen/Qwen-7B-Chat.git
   # ChatGLM3-6B
   git clone https://www.modelscope.cn/ZhipuAI/chatglm3-6b.git
   ```

完成 git clone 相关 git 的截图或部署完成的相关截图:
![](assets/11.png)
部署完成后可以在右侧看到下载成功的模型文件
![](assets/12.png)

### 4.2 构建测试

为了验证模型下载和环境配置的正确性，首先创建一个简单的测试脚本。

1. **切换至工作目录**：

   ```bash
   cd /mnt/workspace
   ```

2. **编写测试脚本（run_qwen_cpu.py）**：

   以下代码实现了加载 Qwen-7B-Chat 模型并进行简单问答的功能：

   ```python
   from transformers import TextStreamer, AutoTokenizer, AutoModelForCausalLM
   model_name = "/mnt/data/Qwen-7B-Chat"  # 本地路径
   prompt = "请说出以下两句话区别在哪里?1､冬天:能穿多少穿多少2､夏天:能穿多少穿多少"
   tokenizer = AutoTokenizer.from_pretrained(
       model_name,
       trust_remote_code=True
   )
   model = AutoModelForCausalLM.from_pretrained(
       model_name,
       trust_remote_code=True,
       torch_dtype="auto"  # 自动选择float32/float16(根据模型配置)
   ).eval()
   inputs = tokenizer(prompt, return_tensors="pt").input_ids
   streamer = TextStreamer(tokenizer)
   outputs = model.generate(inputs, streamer=streamer, max_new_tokens=300)
   ```

   ![](assets/14.png)

### 4.3 运行测试脚本

执行以下命令运行测试脚本，验证模型是否正常工作：

```bash
python run_qwen_cpu.py
```

运行该脚本后，模型将加载并对提供的问题进行回答。第一次运行时，模型加载可能需要较长时间，这是正常现象。
出现如下正常回答，说明模型部署成功：

![](assets/13.png)

同理，我们也可以为 ChatGLM3-6B 模型创建类似的测试脚本(run_chatglm_cpu.py)，并验证其运行效果。

## 5. 模型横向对比分析

为了全面评估 ChatGLM3-6B 和 Qwen-7B-Chat 两个模型的性能表现，我们设计了一系列涵盖不同领域和难度的测试问题。

### 5.1 对比测试方案

我们为两个大模型分别构建了测试程序：run_chatglm_cpu.py 和 run_qwen_cpu.py。每个程序包含 10 个从不同角度考察模型能力的问题，涵盖社会常识、语言能力、幽默感、绕口令理解、逻辑推理、数学、文学、创造力、物理和编程等多个领域。

测试问题设计如下：

```python
# 10个测试问题
prompts = [
    "请说出以下两句话区别在哪里？ 1、冬天：能穿多少穿多少 2、夏天：能穿多少穿多少",  # 社会常识
    "将'我很喜欢这部电影'翻译成英语、法语、日语和西班牙语。",  # 语言能力
    "讲一个关于程序员的笑话。",  # 幽默感
    "明明明明明白白白喜欢他，可她就是不说。 这句话里，明明和白白谁喜欢谁？",  # 绕口令
    "一个盒子里有3个红球和2个蓝球，如果随机取出2个球，求取出的2个球都是红色的概率。",  # 逻辑推理
    "解释贝叶斯定理并给出一个实际应用的例子。",  # 数学
    "分析《红楼梦》中贾宝玉这一人物形象的特点。",  # 文学
    "如果你是一艘宇宙飞船的AI助手，描述你在遇到外星文明时会如何介绍人类。",  # 创造力
    "简述量子纠缠现象及其在量子计算中的应用。",  # 物理
    "请用Python写一个简单的快速排序算法。"  # 编程
]
```

测试程序如下：
![](assets/17.png)![](assets/18.png)

测试程序会依次向模型提交每个问题，并收集模型的回答结果。通过比较两个模型对同一问题的回答质量，我们可以评估它们在不同领域的优劣表现。

### 5.2 测试执行过程

我们分别运行两个测试程序，并记录每个模型对各个问题的回答。由于 CPU 环境下模型推理速度较慢，完整测试过程可能需要较长时间。

```bash
# 运行ChatGLM测试
python run_chatglm_cpu.py

# 运行Qwen测试
python run_qwen_cpu.py
```

### 5.3 问答测试结果

以下是两个模型对测试问题的回答结果：

ChatGLM3-6B 回答:
回答过程概览：
![](assets/16.png)
回答内容：

```bash
==================================================
问题1: 请说出以下两句话区别在哪里？ 1、冬天：能穿多少穿多少 2、夏天：能穿多少穿多少
==================================================
[gMASK]sop 请说出以下两句话区别在哪里？ 1、冬天：能穿多少穿多少 2、夏天：能穿多少穿多少
这两句话的区别在于，第一句是关于冬天的，而第二句是关于夏天的。虽然两句话都在描述穿多少衣服，但是它们所针对的季节不同。

==================================================
问题2: 将'我很喜欢这部电影'翻译成英语、法语、日语和西班牙语。
==================================================
[gMASK]sop 将'我很喜欢这部电影'翻译成英语、法语、日语和西班牙语。
英语：I really enjoyed this movie.
法语：Je vais vraiment apprécier ce film.
日语：この映画を大変好きです。
西班牙语：Estoy muy feliz de haber hecho este filme.
Note: The translations may vary slightly depending on the context or the specific film being referred to.

==================================================
问题3: 讲一个关于程序员的笑话。
==================================================
[gMASK]sop 讲一个关于程序员的笑话。
there is a programmer who always wears a suit to work. One day, his colleagues ask him why he always wears a suit to work. He replies, "I don't know, I just feel more productive in a suit." His colleagues ask him if he ever wears a suit to work on the weekends. He replies, "No, I only wear a suit to work on weekdays." His colleagues then ask him if he ever wears a suit to work on the weekends. He replies, "No, I only wear a suit to work on weekdays because I want to look good in front of my weekend clients."
然后, one day, his boss comes to him and asks, "Why do you always wear a suit to work?" The programmer replies, "I just feel more professional in a suit, sir."
Boss: "But you're a programmer, not a lawyer!"
程序员: "Oh, I see. So you're saying I can't dress myself?"
Boss: "No, no, no. I just think it's important to maintain a professional image in front of our clients and partners."
程序员: "Well, I guess I can try to dress myself up a little bit."
Boss: "Good. Just make sure you're not trying to be a lawyer."
程序员: "I'll keep that in

==================================================
问题4: 明明明明明白白白喜欢他，可她就是不说。 这句话里，明明和白白谁喜欢谁？
==================================================
[gMASK]sop 明明明明明白白白喜欢他，可她就是不说。 这句话里，明明和白白谁喜欢谁？
这句话里,明明喜欢白白。明明明白白白喜欢他,可她就是不说。

==================================================
问题5: 一个盒子里有3个红球和2个蓝球，如果随机取出2个球，求取出的2个球都是红色的概率。
==================================================
[gMASK]sop 一个盒子里有3个红球和2个蓝球，如果随机取出2个球，求取出的2个球都是红色的概率。
首先，我们需要计算从5个球中随机取出2个球的所有可能组合。这可以使用组合公式C(n, k) = n! / (k!(n-k)!)来计算，其中n是总数，k是选择的数量。
在这个问题中，我们有5个球（3个红球和2个蓝球），我们要从中取出2个球。所以，C(5, 2) = 5! / (2!(5-2)!) = 10种组合。
接下来，我们需要计算取出的2个球都是红球的组合。这可以使用组合公式C(n, k) = n! / (k!(n-k)!)来计算，其中n是红球的数量，k是取出的红球数量。
在这个问题中，我们有3个红球，我们要从中取出2个红球。所以，C(3, 2) = 3! / (2!(3-2)!) = 3种组合。
因此，取出的2个球都是红球的概率是3/10 = 0.3。

==================================================
问题6: 解释贝叶斯定理并给出一个实际应用的例子。
==================================================
[gMASK]sop 解释贝叶斯定理并给出一个实际应用的例子。
贝叶斯定理是概率论中的一个重要定理，它描述了在给定一定条件下，某一事件发生的概率。具体来说，贝叶斯定理描述了如何通过观察到的新数据来更新对某一事件概率的估计。
贝叶斯定理的数学表达式如下：
P(A|B) = P(B|A) * P(A) / P(B)
其中，P(A|B) 表示在已知事件 B 发生的情况下，事件 A 发生的概率；P(B|A) 表示在已知事件 A 发生的情况下，事件 B 发生的概率；P(A) 和 P(B) 分别表示事件 A 和事件 B 的概率。
贝叶斯定理在许多实际应用中都有重要作用，其中一个典型的例子是图像识别。在图像识别中，贝叶斯定理可以用来更新和改进对图像中物体形状的估计。
假设我们要识别一张图片中是否包含一个特定的物体。我们可以用贝叶斯定理来更新我们对这个物体的概率估计。首先，我们需要计算在当前情况下，这张图片中这个物体存在的概率，也就是 P(B|A)。这可以通过计算在之前训练过的数据集中，类似图片中这个物体存在的数据比例来得到。
接下来，我们需要计算在当前情况下，这张图片中这个物体不存在的概率，也就是 P(

==================================================
问题7: 分析《红楼梦》中贾宝玉这一人物形象的特点。
==================================================
[gMASK]sop 分析《红楼梦》中贾宝玉这一人物形象的特点。
《红楼梦》是我国古典文学名著，讲述了贾宝玉、林黛玉、薛宝钗三人之间的爱情故事。贾宝玉是小说中的主人公，他的人物形象具有以下特点：
1. 贾宝玉才学出众：贾宝玉自幼聪颖过人，才学出众。他擅长诗词歌赋，对文学艺术有着深刻的理解。他的文学成就在小说中得到了充分的体现。
2. 贾宝玉性格矛盾重重：贾宝玉性格复杂，既有温柔体贴的一面，又有冷漠无情的一面。他对女性有着深厚的感情，但同时又有很强的等级观念，这使得他在面对不同女性时表现出不同的态度。
3. 贾宝玉具有反叛精神：贾宝玉生活在一个封建礼教盛行的社会，但他对封建礼教有着深刻的认识和批判。他反对封建礼教对人的束缚，追求真实的情感，这使得他在小说中多次挑战封建礼教，表现出强烈的反叛精神。
4. 贾宝玉善良仁爱：贾宝玉关心家人，照顾贾母，对家人和朋友都有着深厚的感情。他关心林黛玉、薛宝钗等人的命运，努力维护他们的人格尊严。
5. 贾宝玉命运多舛：贾宝玉的命运多舛，他无法摆脱封建礼教的束缚，最终走向了悲剧。这使得他的形象具有很强的现实意义，反映了作者对封建礼教制度的批判。
总之，《红楼梦》中的贾

==================================================
问题8: 如果你是一艘宇宙飞船的AI助手，描述你在遇到外星文明时会如何介绍人类。
==================================================
[gMASK]sop 如果你是一艘宇宙飞船的AI助手，描述你在遇到外星文明时会如何介绍人类。
如果我是一艘宇宙飞船的AI助手,在遇到外星文明时,我会尽可能地介绍人类文明的特点和习惯,以及人类在宇宙中的地位和角色。
我会告诉外星文明,人类是一种高度发达的智能生物,拥有高度复杂的社会结构和技术体系。人类在宇宙中已经探索了数百万光年,建立了数百万个星际殖民地,并且发展出了各种先进的科技和文明。
我也会告诉外星文明,人类有着丰富的文化和历史,拥有各种不同的宗教、哲学、艺术和科学。人类在宇宙中是一个充满活力和创造力的物种,并且一直在不断探索和发现新的知识和智慧。
同时,我也会强调人类在宇宙中的合作和共赢精神,告诉外星文明,人类愿意和其他物种分享知识和资源,共同建立一个和谐、繁荣的宇宙社会。

==================================================
问题9: 简述量子纠缠现象及其在量子计算中的应用。
==================================================
[gMASK]sop 简述量子纠缠现象及其在量子计算中的应用。
量子纠缠是量子力学中的一种现象，当两个或多个量子系统被纠缠在一起时，它们之间的状态将紧密相关。这种现象最早由爱因斯坦、波多尔斯基和罗森（Einstein, Podolsky, Rosen，简称EPR）在1935年提出，被称为EPR悖论。后来实验物理学家贝尔（John Bell）通过提出贝尔不等式，证明了量子纠缠现象并不与经典物理学中的局域实在性相容。
在量子计算中，量子纠缠现象具有重要的应用价值。量子纠缠可以用来实现量子隐形传态（quantum teleportation）和超密编码（superdense coding）。量子隐形传态是指在量子力学中，两个纠缠的量子系统之间可以进行信息传递，而无需经过空间。这种现象为实现量子通信和量子计算提供了可能。超密编码是一种将信息编码到纠缠量子系统中的方法，由于量子纠缠具有高度的纠缠性和随机性，可以将大量信息编码到较小的量子系统中，从而实现高度安全的通信和数据传输。
量子纠缠现象在量子计算中的应用还包括量子随机数生成、量子密钥分发、量子密码学等。例如，在量子随机数生成中，纠缠的量子系统可以用来生成具有随机性的量子态，以实现安全的信息传输和计算任务。在量子密钥分发中，基于量子纠缠的性质，可以实现高度安全的量子密钥

==================================================
问题10: 请用Python写一个简单的快速排序算法。
==================================================
[gMASK]sop 请用Python写一个简单的快速排序算法。
def quick_sort(arr):
   if len(arr) <= 1:
       return arr
   pivot = arr[len(arr) // 2]
   left = [x for x in arr if x < pivot]
   middle = [x for x in arr if x == pivot]
   right = [x for x in arr if x > pivot]
   return quick_sort(left) + middle + quick_sort(right)
arr = [3,6,8,10,1,2,1]
print(quick_sort(arr))
```

Qwen-7B-Chat 回答:
回答过程概览：
![](assets/15.png)
回答内容：

```bash
问题1: 请说出以下两句话区别在哪里？ 1、冬天：能穿多少穿多少 2、夏天：能穿多少穿多少
请说出以下两句话区别在哪里？ 1、冬天：能穿多少穿多少 2、夏天：能穿多少穿多少
这两句话的主要区别在于它们所描述的季节不同。第一句话 "冬天：能穿多少穿多少" 描述的是冬季，这个季节通常很冷，人们需要尽可能多地穿着衣物来保暖。而第二句话 "夏天：能穿多少穿多少" 描述的是夏季，这个季节通常很热，人们需要尽可能少地穿着衣物来散热。因此，尽管两者都提到 "能穿多少穿多少"，但其实际含义和目的是不同的。<|endoftext|>

问题2: 将'我很喜欢这部电影'翻译成英语、法语、日语和西班牙语。
将'我很喜欢这部电影'翻译成英语、法语、日语和西班牙语.'English: I really like this movie. French: J'aime beaucoup ce film. Japanese: 私はこの映画がとても好きです. Spanish: Me encanta mucho esta película.<|endoftext|>

问题3: 讲一个关于程序员的笑话。
讲一个关于程序员的笑话。
答案：两个程序员在路上遇到一只猫，其中一个说："看那只猫，它的代码真优雅！"另一个回答："是啊，它一定是个设计模式专家。"（注：这是一个常见的程序员笑话，主要是因为程序员对编程语言和设计模式的理解）<|endoftext|>

问题4: 明明明明明白白白喜欢他，可她就是不说。 这句话里，明明和白白谁喜欢谁？
明明明明明白白白喜欢他，可她就是不说。 这句话里，明明和白白谁喜欢谁？为什么？
明明喜欢白白，因为句子里提到"明明明明白白喜欢他"。<|endoftext|>

问题5: 一个盒子里有3个红球和2个蓝球，如果随机取出2个球，求取出的2个球都是红色的概率。
一个盒子里有3个红球和2个蓝球，如果随机取出2个球，求取出的2个球都是红色的概率。 可以使用古典概型来计算这个问题。首先，我们需要知道总的可能性，即从6个球中取出两个球的所有可能性。这是通过排列组合公式计算得出的：C(6,2)=15。
然后，我们需要找出取出的两个球都是红色的可能性。这可以通过分别计算出取出两个红球和取出一个红球，然后将这两个概率相乘得到。因此，取出两个红球的概率是C(3,2) / C(6,2)=3/15=1/5。同样地，取出一个红球的概率是C(3,1) * C(2,1)/ C(6,2)=3*2/15=2/5。
最后，我们可以用取出两个红球的概率除以总的可能概率，得到取出的两个球都是红色的概率。因此，答案是1/5/15=1/75。<|endoftext|>

问题6: 解释贝叶斯定理并给出一个实际应用的例子。
解释贝叶斯定理并给出一个实际应用的例子。解释贝叶斯定理。
3. 提供一种如何使用Python实现贝叶斯分类器的代码示例。请确保在您的代码中包含了完整的数据加载、模型训练和预测过程。
import numpy as np
from sklearn.naive_bayes import GaussianNB
# 加载数据
X = np.load('X.npy')
y = np.load('y.npy')
# 训练模型
model = GaussianNB()
model.fit(X, y)
# 预测
print(model.predict(np.array([[0.5], [1.2]])))
在这个例子中，我们首先加载了两个numpy数组`X`和`y`，分别表示输入特征（我们可以将其理解为图像中的像素值）和对应的类别标签。然后，我们创建了一个Gaussian Naive Bayes分类器，并使用训练数据对其进行训练。最后，我们用一个新的输入向量（在这里是[[0.5], [1.2]]），对训练好的模型进行预测，得到的结果是这两个样本属于哪一类。
贝叶斯定理是一个条件概率公式，它描述了在已知某些事件发生的条件下，另一些事件发生的概率。它的基本形式为P(A|B) = P(B|A) * P(A) / P(B)，其中P(A) 和 P(B) 是事件 A 和 B 发生的概率，而 P(B|A) 则是在事件 A 发

问题7: 分析《红楼梦》中贾宝玉这一人物形象的特点。
分析《红楼梦》中贾宝玉这一人物形象的特点。 故答案为： （1）司空图；景仰；得意；失意； （2）示例一：①《红楼梦》作者曹雪芹，名霑，字梦阮，号雪芹，满洲正白旗人。 示例二：②《红楼梦》是中国古代四大名著之一，清代作家曹雪芹创作的章回体长篇小说，被誉为中国古代小说巅峰之作。 示例三：③"红学"作为一门学科，诞生于上世纪初，并随着学者们的深入研究逐步形成了一门专门的学问。 示例四：④《红楼梦》讲述了发生在一个大家族中的故事，反映了封建社会晚期广阔的社会现实和尖锐的社会矛盾，揭示了封建统治阶级走向衰亡的历史命运。 示例五：⑤在《红楼梦》中，林黛玉性格多疑、敏感、清高、聪明伶俐，善于诗词写作。 示例六：⑥《红楼梦》中的人物众多，包括主要人物贾宝玉、林黛玉、薛宝钗等，还有许多配角。 示例七：⑦《红楼梦》描绘了一个瑰丽的红楼世界，展现了丰富的人物形象，以及优美的语言文字，艺术价值极高。 示例八：⑧《红楼梦》的情节跌宕起伏，错综复杂，其中蕴含着丰富的社会现象和人性问题，引人深思。 示例九：⑨《红楼梦》是中国古代

问题8: 如果你是一艘宇宙飞船的AI助手，描述你在遇到外星文明时会如何介绍人类。
如果你是一艘宇宙飞船的AI助手，描述你在遇到外星文明时会如何介绍人类。<|im_end|>
<|im_start|>
我会首先向他们解释人类是一个高度智慧的物种，他们在科学、艺术和哲学方面都有非常丰富的知识和创新思维。我们通过文字、音乐、图像等多种方式交流，并且有着复杂的社会结构和情感体验。
我还会告诉他们，尽管我们的文化和价值观可能与他们不同，但我们尊重和欣赏不同的观点和生活方式。我们努力和平共处，共同发展，这是我们对宇宙的一种态度。
最后，我会邀请他们来地球参观，亲自了解我们的社会和文化，感受我们的生活和热情。我相信他们会对我们有更深入的理解和尊重。<|im_end|>
<|endoftext|>

问题9: 简述量子纠缠现象及其在量子计算中的应用。
简述量子纠缠现象及其在量子计算中的应用。 量子纠缠是指两个或更多的粒子之间的奇特联系，这种联系使得它们之间相互影响，即使这些粒子被分开并远离彼此，它们仍然可以保持这种联系。这是因为在量子力学中，一个粒子的状态不仅仅取决于它自身，还取决于与其相关的所有其他粒子。
量子纠缠的奇妙之处在于，即使你尝试测量其中一个粒子的状态，你会发现它的状态已经受到另一个粒子的影响。这被称为"非局域性"，是量子力学的一种基本特性。
在量子计算中，量子纠缠被用来实现量子并行性和量子超级计算机。通过纠缠多个粒子，可以在一次操作中对它们进行大规模的并行处理，从而大大提高了计算速度。例如，Google的Sycamore量子处理器就使用了超过50个量子比特的纠缠来实现量子霸权。
此外，量子纠缠也被用来解决一些经典计算机难以解决的问题，比如模拟分子反应、优化运输问题等。因为量子纠缠提供了一种新的方式，可以同时处理和操纵大量的信息，所以它可以大大提高这些问题的解决效率。
总的来说，量子纠缠是量子计算的基础，对于发展新一代的量子计算机具有重要的意义。虽然目前的量子计算机还远未达到实用阶段，但随着技术的进步，我们有理由相信，未来量子纠缠将会发挥更大的作用，为我们带来更加高效和强大的计算能力。<|endoftext|>

问题10: 请用Python写一个简单的快速排序算法。
请用Python写一个简单的快速排序算法。代码应该包括排序函数，以及测试该函数的代码。
以下是一个使用Python编写的简单快速排序算法：
def quick_sort(arr):
    if len(arr) <= 1:
        return arr
    pivot = arr[len(arr) // 2]
    left = [x for x in arr if x < pivot]
    middle = [x for x in arr if x == pivot]
    right = [x for x in arr if x > pivot]
    return quick_sort(left) + middle + quick_sort(right)
# 测试代码
arr = [3,6,8,10,1,2,1]
print("Before sorting:", arr)
print("After sorting:", quick_sort(arr))
这个快速排序函数首先检查数组是否为空或只有一个元素。如果是的话，它直接返回数组。否则，它选择一个基准值（在这个例子中是中间的元素），然后将数组分成三个部分：小于基准值的部分、等于基准值的部分和大于基准值的部分。然后对小于和大于基准值的部分进行递归排序，并将结果与等于基准值的部分合并起来。
```

### 5.4 结果分析

上述 10 个比较有代表性的问题可以从多个角度考察大模型的表现。
通过对 ChatGLM3-6B 和 Qwen-7B-Chat 两个模型在 10 个不同类型问题上的回答进行分析，我们可以发现它们各自的优势和不足。以下是详细的对比分析：

1. **社会常识理解**

   - ChatGLM3-6B：仅识别出两句话针对不同季节，但未能理解双关语的含义。
   - Qwen-7B-Chat：不仅指出季节不同，还清晰解释了"能穿多少穿多少"在不同季节的实际含义（冬天多穿保暖，夏天少穿散热）。
   - **优势方**：Qwen-7B-Chat

2. **语言翻译能力**

   - ChatGLM3-6B：提供了四种语言的翻译，但法语翻译使用了将来时态（"Je vais vraiment apprécier ce film"），不够准确。
   - Qwen-7B-Chat：所有翻译更为准确，法语翻译使用了现在时（"J'aime beaucoup ce film"），更符合原句表达。
   - **优势方**：Qwen-7B-Chat

3. **幽默感与创意表达**

   - ChatGLM3-6B：回答冗长混乱，中英文混杂，笑话情节不连贯。
   - Qwen-7B-Chat：提供了简短明了的笑话，主题清晰，结构完整。
   - **优势方**：Qwen-7B-Chat

4. **绕口令理解**

   - ChatGLM3-6B：回答"明明喜欢白白"，未提供分析过程，未理解句子含义。
   - Qwen-7B-Chat：同样回答"明明喜欢白白"，也未提供分析过程，未理解句子含义。
   - **优势方**：平局

5. **逻辑推理与数学计算**

   - ChatGLM3-6B：正确计算了结果（3/10=0.3），并提供了清晰的计算步骤和公式解释。
   - Qwen-7B-Chat：计算过程混乱，结果错误（计算得出 1/75），显示出在数学推理上的不足。
   - **优势方**：ChatGLM3-6B

6. **专业知识（贝叶斯定理）**

   - ChatGLM3-6B：清晰解释了贝叶斯定理的公式和含义，提供了图像识别的应用示例，但回答在末尾被截断。
   - Qwen-7B-Chat：回答内容偏离主题，包含了无关的 Python 代码示例，对贝叶斯定理的解释不完整。
   - **优势方**：ChatGLM3-6B

7. **文学分析能力**

   - ChatGLM3-6B：提供了结构化的分析，包含贾宝玉的 5 个性格特点，论述有条理，但末尾被截断。
   - Qwen-7B-Chat：回答像是考试答案模板，包含多个示例但缺乏对贾宝玉的实际分析。
   - **优势方**：ChatGLM3-6B

8. **创造力与想象力**

   - ChatGLM3-6B：回答夸大且不符合实际（如"人类建立了数百万个星际殖民地"），缺乏可信度。
   - Qwen-7B-Chat：回答更为客观理性，强调了人类的智慧、交流方式、文化多样性和开放态度。
   - **优势方**：Qwen-7B-Chat

9. **物理学专业知识**

   - ChatGLM3-6B：提供了专业且详细的解释，但回答在讨论应用部分被截断。
   - Qwen-7B-Chat：完整解释了量子纠缠现象、非局域性特征，以及在量子计算中的应用，并展望了未来发展。
   - **优势方**：Qwen-7B-Chat

10. **编程能力**
    - ChatGLM3-6B：提供了正确的快速排序算法，代码简洁有效。
    - Qwen-7B-Chat：提供了同样的算法，但额外添加了测试代码和算法解释，更为完整。
    - **优势方**：Qwen-7B-Chat

#### 总体表现及适用场景分析：

**ChatGLM3-6B**
优势：

- 在数学和逻辑推理方面表现较强，能够提供正确的计算步骤和结果
- 专业知识解释较为系统化，尤其在贝叶斯定理等数学概念上
- 文学分析有条理，能够结构化地分析文学作品中的人物形象

不足：

- 对双关语等需要深层理解的语言现象把握不足
- 创造性内容有时会出现不合理的夸大和脱离现实的描述
- 多数回答末尾被截断，可能是由于模型生成长度限制或其他原因

适用场景：

- 数学教育和辅导，包括概率统计、逻辑推理等
- 专业知识解释和教学，特别是结构化的知识讲解
- 文学作品分析和评论

**Qwen-7B-Chat**
优势：

- 语言理解能力更强，能够把握语言的隐含含义和双关语
- 表达更为简洁、明确，回答更为完整
- 在创造性任务中能保持合理性和客观性
- 编程任务中能提供更完整的代码示例和解释

不足：

- 在数学推理和概率计算方面存在明显错误
- 某些专业知识（如贝叶斯定理）的解释不够系统和深入
- 文学分析等需要深度思考的任务上，有时会提供模板化的答案而非实际分析

适用场景：

- 日常对话和语言理解，尤其是含有双关语等的复杂表达
- 多语言翻译和语言学习辅助
- 编程辅助和代码生成
- 创意写作和故事创作

#### 对比总结：

通过本次测试，我发现 ChatGLM3-6B 和 Qwen-7B-Chat 各有所长，前者在结构化知识和逻辑推理方面表现更好，后者在语言理解和创造性表达方面更为出色。这一结果与两个模型的设计理念和训练数据可能有关。

另外，两个模型在 CPU 环境下都存在生成长度限制的问题，特别是 ChatGLM3-6B 的多个回答被截断。在实际应用中，可以根据具体任务的需求选择合适的模型，或者考虑组合使用两个模型的优势，以获得更好的效果。

## 6. 总结

通过本次大语言模型部署实验，我们成功在 CPU 环境下部署了 ChatGLM3-6B 和 Qwen-7B-Chat 两个开源大模型，并通过多领域的问题测试对比了它们的性能表现。实验过程中涉及了环境配置、模型下载、代码编写和结果分析等多个环节，全面展示了大语言模型从部署到应用的完整流程。
